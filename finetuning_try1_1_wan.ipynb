{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6765d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 설치\n",
    "!pip install trl peft accelerate bitsandbytes datasets transformers wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6377b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from huggingface_hub import login\n",
    "\n",
    "# 0. Hugging Face 토큰 직접 로그인\n",
    "login(token=\"a\")  \n",
    "\n",
    "# 1. wandb 초기화\n",
    "wandb.init(project=\"GenBot\", name=\"exaone-finetuning1\")\n",
    "\n",
    "# 2. 모델 및 토크나이저 설정\n",
    "model_name   = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "dataset_path = \"card_consult_finetune_messages.jsonl\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. LoRA 설정\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 4. 데이터 처리 함수\n",
    "def format_data(input):\n",
    "    formatted_dialogue = {\"messages\": []}\n",
    "    prev_role, temp_content = None, \"\"\n",
    "    system_prompt = {\"role\": \"system\", \"content\": \"당신은 친절하고 정확한 고객상담 챗봇입니다.\"}\n",
    "    formatted_dialogue[\"messages\"].append(system_prompt)\n",
    "    for message in input[\"messages\"]:\n",
    "        role, content = message[\"role\"], message[\"content\"].strip()\n",
    "        if role == prev_role:\n",
    "            temp_content += \" \" + content\n",
    "        else:\n",
    "            if temp_content:\n",
    "                formatted_dialogue[\"messages\"].append({\"role\": prev_role, \"content\": temp_content.strip()})\n",
    "            temp_content, prev_role = content, role\n",
    "    if temp_content:\n",
    "        formatted_dialogue[\"messages\"].append({\"role\": prev_role, \"content\": temp_content.strip()})\n",
    "    return formatted_dialogue\n",
    "\n",
    "# 5. 데이터셋 로드 및 전처리\n",
    "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "dataset = dataset.map(format_data, batched=False)\n",
    "\n",
    "def preprocess(example, tokenizer):\n",
    "    in_ids, lbls = [], []\n",
    "    msgs = example[\"messages\"]\n",
    "    for i, msg in enumerate(msgs):\n",
    "        if msg[\"role\"] != \"assistant\":\n",
    "            continue\n",
    "        ctx = \"\"\n",
    "        for prev in msgs[:i]:\n",
    "            tag = \"<|user|>\" if prev[\"role\"] == \"user\" else \"<|assistant|>\"\n",
    "            ctx += tag + prev[\"content\"] + tokenizer.eos_token\n",
    "        ctx += \"<|assistant|>\"\n",
    "        resp = msg[\"content\"] + tokenizer.eos_token\n",
    "        ctx_ids  = tokenizer(ctx,  add_special_tokens=False).input_ids\n",
    "        resp_ids = tokenizer(resp, add_special_tokens=False).input_ids\n",
    "        in_ids.append(ctx_ids + resp_ids)\n",
    "        lbls.append([-100] * len(ctx_ids) + resp_ids)\n",
    "    return {\"input_ids\": in_ids, \"labels\": lbls}\n",
    "\n",
    "proc = dataset.map(\n",
    "    preprocess,\n",
    "    batched=False,\n",
    "    remove_columns=[\"messages\"],\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    ")\n",
    "\n",
    "# 6. Flatten\n",
    "all_inputs, all_labels = [], []\n",
    "for x, y in zip(proc[\"input_ids\"], proc[\"labels\"]):\n",
    "    all_inputs.extend(x)\n",
    "    all_labels.extend(y)\n",
    "train_ds = Dataset.from_dict({\"input_ids\": all_inputs, \"labels\": all_labels})\n",
    "\n",
    "# 7. Collate 함수\n",
    "def collate_fn(batch):\n",
    "    return tokenizer.pad(\n",
    "        {\n",
    "            \"input_ids\": [ex[\"input_ids\"] for ex in batch],\n",
    "            \"labels\":    [ex[\"labels\"]    for ex in batch],\n",
    "        },\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# 8. 학습 설정 (wandb + Hub)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./lora_exaone\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"exaone-chatbot1\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"jangsukim/exaone_finetuning1\",\n",
    "    hub_strategy=\"every_save\",\n",
    ")\n",
    "\n",
    "# 9. Trainer 정의\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# 10. 학습 시작 & 허브 업로드\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
