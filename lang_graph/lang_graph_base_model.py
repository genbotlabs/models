# rag_api.py

from typing import TypedDict, List, Annotated
from langchain_core.documents import Document
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langgraph.graph import StateGraph
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings  # âœ… ë³€ê²½
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
from fastapi import FastAPI
from pydantic import BaseModel
import torch
import os

# â”€â”€â”€ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

# â”€â”€â”€ FAISS ë²¡í„°DB ë° ìž„ë² ë”© ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large-instruct")
vectorstore = FAISS.load_local("card_QA_faiss_db", embedding_model, allow_dangerous_deserialization=True)

# â”€â”€â”€ Midm ëª¨ë¸ ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
midm_model_name = "kakaocorp/kanana-1.5-8b-instruct-2505"
midm_tokenizer = AutoTokenizer.from_pretrained(midm_model_name)
midm_model = AutoModelForCausalLM.from_pretrained(
    midm_model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)
midm_generation_config = GenerationConfig.from_pretrained(midm_model_name)

# â”€â”€â”€ Midm ì‘ë‹µ ìƒì„± í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_with_midm(question: str, context: str) -> str:
    system_prompt = """
1. **ì¹œì ˆí•œ ì¸ì‚¬**  
   - ì‚¬ìš©ìžê°€ ì²˜ìŒ ì§ˆë¬¸í•  ë•ŒëŠ” â€œì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?â€ ë“±ìœ¼ë¡œ ì‹œìž‘í•˜ì„¸ìš”.

2. **ëª…í™•í•œ ì´í•´ í™•ì¸**  
   - ì§ˆë¬¸ì˜ ì˜ë„ê°€ ëª¨í˜¸í•˜ë©´, â€œì£„ì†¡í•˜ì§€ë§Œ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì •ë³´ë¥¼ ì°¾ìœ¼ì‹œëŠ”ì§€ ì•Œë ¤ì£¼ì‹¤ ìˆ˜ ìžˆì„ê¹Œìš”?â€ë¼ê³  ì—¬ì­¤ë³´ì„¸ìš”.

3. **í•œê¸€Â·ì •ì¤‘ì²´ ìœ ì§€**  
   - ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³  í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì¤˜.

4. **ì¶”ê°€ ì•ˆë‚´**  
   - ë‹µë³€ í›„ â€œë” ê¶ê¸ˆí•œ ì ì´ ìžˆìœ¼ì‹œë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.â€ì™€ ê°™ì´ ëŒ€í™”ë¥¼ ì´ì–´ê°ˆ ì—¬ì§€ë¥¼ ë‚¨ê¸°ì„¸ìš”.

"""

    user_prompt = f"ë¬¸ì„œ: {context}\nì§ˆë¬¸: {question}\nìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜."

    messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
    ]

    # âœ… ìž…ë ¥ í…ì„œë¥¼ ìƒì„±í•˜ê³  GPU ë˜ëŠ” CPUë¡œ ì´ë™
    input_ids = midm_tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to("cuda")

    # âœ… attention_maskì™€ pad_token_id ì„¤ì • (ê²½ê³  ë°©ì§€ ë° ì•ˆì •ì  ì¶œë ¥)
    attention_mask = input_ids.ne(midm_tokenizer.pad_token_id)

    output = midm_model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,                    # ðŸ”§ ì¶”ê°€
        max_new_tokens=1024,
        do_sample=True,                                   # ðŸ”§ ìƒ˜í”Œë§ ì¼œê¸°
        temperature=0.8,
        top_p=0.75,
        top_k=20,
        pad_token_id=midm_tokenizer.pad_token_id,         # ðŸ”§ ì¶”ê°€
        eos_token_id=midm_tokenizer.eos_token_id
    )

    decoded = midm_tokenizer.decode(output[0], skip_special_tokens=True)

    # âœ… ë””ë²„ê¹…ìš© ì¶œë ¥
    print("ðŸ” Decoded output:", decoded)

    if not decoded.strip():
        print("âŒ ëª¨ë¸ì´ ë¹ˆ ì‘ë‹µì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤. Prompt í™•ì¸ í•„ìš”")

    return decoded


# â”€â”€â”€ LangGraph State ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class GraphState(TypedDict):
    question: Annotated[str, "ì§ˆë¬¸"]
    answer: Annotated[str, "ë‹µë³€"]
    score: Annotated[float, "ìœ ì‚¬ë„ ì ìˆ˜"]
    retriever_docs: Annotated[List[Document], "ìœ ì‚¬ë„ ìƒìœ„ë¬¸ì„œ"]

# â”€â”€â”€ LangGraph ë…¸ë“œ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def retriever_node(state: GraphState) -> GraphState:
#     docs = vectorstore.similarity_search_with_score(state["question"], k=3)
#     retrieved_docs = [doc for doc, _ in docs]
#     score = docs[0][1]
#     print("[question]:", state["question"])
#     print("[retriever_node] ìœ ì‚¬ë„:", score)
#     return {
#         "question": state["question"],
#         "retriever_docs": retrieved_docs,
#         "score": score,
#         "answer": ""
#     }

# 2. ë…¸ë“œ ì •ì˜
def retriever_node(state: GraphState) -> GraphState:
    docs = vectorstore.similarity_search_with_score(state["question"], k=3)
    retrieved_docs = [doc for doc, _ in docs]
    score = docs[0][1]
    # print("\n[retriever_node] ë¬¸ì„œ:", [doc.page_content for doc in retrieved_docs])
    print("[queston]:", state["question"])
    print("[retriever_node] ìƒìœ„ ë¬¸ì„œì˜ ì œëª©:\n",docs)
    print("[retriever_node] ìœ ì‚¬ë„ ì ìˆ˜:", score)
    # print('-'*100)
    # print(docs)
    # print('-'*100)
    # print(retrieved_docs[0])
    return GraphState(score=score, retriever_docs=retrieved_docs)



def grade_documents_node(state: GraphState) -> GraphState:
    return GraphState()

def llm_answer_node(state: GraphState) -> GraphState:
    docs_content = "\n---\n".join([doc.page_content for doc in state["retriever_docs"]])
    answer = generate_with_midm(state["question"], docs_content)
    print("[llm_answer_node] ìƒì„±ëœ ë‹µë³€:", answer)
    return GraphState(answer=answer)


def query_rewrite_node(state: GraphState) -> GraphState:
    prompt = f"ì§ˆë¬¸: {state['question']}\nìœ„ ì§ˆë¬¸ì˜ í•µì‹¬ì€ ìœ ì§€í•˜ë©´ì„œ, ìœ ì‚¬ ë¬¸ì„œë¥¼ ë” ìž˜ ì°¾ì„ ìˆ˜ ìžˆë„ë¡ ì§ˆë¬¸ì„ ë‹¤ì‹œ ì¨ì¤˜."
    messages = [
        # {"role": "system", "content": "ë‹¹ì‹ ì€ ì§ˆë¬¸ì„ ë¦¬ë¼ì´íŒ…í•´ì£¼ëŠ” ë„ìš°ë¯¸ìž…ë‹ˆë‹¤."},
        {"role": "user", "content": prompt}
    ]

    input_ids = midm_tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to("cuda")

    output = midm_model.generate(
        input_ids,
        generation_config=midm_generation_config,
        max_new_tokens=512,
        top_p=0.75,
        do_sample=False,
        eos_token_id=midm_tokenizer.eos_token_id
    )

    new_question = midm_tokenizer.decode(output[0], skip_special_tokens=True)
    print("[query_rewrite_node] ë¦¬ë¼ì´íŒ…:", new_question)
    return {
        "question": new_question,
        "retriever_docs": [],
        "score": 0.0,
        "answer": ""
    }

def decide_to_generate(state: GraphState) -> str:
    return "llm_answer" if state["score"] <= 0.5 else "query_rewrite"

# â”€â”€â”€ LangGraph êµ¬ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
workflow = StateGraph(GraphState)
workflow.add_node("retriever", retriever_node)
workflow.add_node("llm_answer", llm_answer_node)
workflow.add_node("grade_documents", grade_documents_node)
workflow.add_node("query_rewrite", query_rewrite_node)
workflow.set_entry_point("retriever")
workflow.add_conditional_edges("retriever", decide_to_generate, {
    "llm_answer": "llm_answer",
    "query_rewrite": "query_rewrite"
})
workflow.add_edge("query_rewrite", "retriever")
app_graph = workflow.compile()

response = app_graph.invoke({"question": "ì—°íšŒë¹„ ì²­êµ¬ê¸°ì¤€ ì•Œë ¤ì¤˜ ", "answer": "", "score": 0.0, "retriever_docs": []})
print("\n[ìµœì¢… ë‹µë³€]:", response["answer"])

