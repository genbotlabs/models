{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6efa50c",
   "metadata": {},
   "outputs": [],
   "source": [
    " pip install wandb transformers datasets bitsandbytes peft evaluate scikit-learn bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f50d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c18c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# 0. 메모리 단편화 완화\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "# 1. Quantization 설정\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 2. 양자화된 SOLAR 모델 로드\n",
    "model_name = \"upstage/SOLAR-10.7B-v1.0\"\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 3. LoRA 어댑터 설정 & 부착\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "# 4. 캐시 끄기 & (일단) gradient checkpointing 비활성화\n",
    "model.config.use_cache = False\n",
    "# model.gradient_checkpointing_enable()   # 메모리 여유 있으면 켜세요\n",
    "\n",
    "# 5. **Train 모드 활성화** — 여기서 반드시 호출해야 합니다!\n",
    "model.train()\n",
    "\n",
    "# 6. 학습 가능한 파라미터 수 출력 (디버그)\n",
    "total, trainable = 0, 0\n",
    "for n, p in model.named_parameters():\n",
    "    num = p.numel()\n",
    "    total += num\n",
    "    if p.requires_grad:\n",
    "        trainable += num\n",
    "        print(f\"[TRAINABLE] {n} ({num} params)\")\n",
    "print(f\"▶︎ Trainable params: {trainable:,} / {total:,}\")\n",
    "\n",
    "# 7. 토크나이저 세팅\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|user|>\", \"<|assistant|>\"]})\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "\n",
    "# 8. 데이터 로드 & 전처리\n",
    "raw = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"card_consult_finetune_messages.jsonl\"},\n",
    "    split=\"train\"\n",
    ")\n",
    "MAX_LEN = 256\n",
    "\n",
    "def format_data(example):\n",
    "    # 같은 역할(role)이 연속된 메시지 합치기 + system 프롬프트 추가\n",
    "    formatted = {\"messages\": []}\n",
    "    formatted[\"messages\"].append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"당신은 친절하고 정확한 고객상담 챗봇입니다.\"\n",
    "    })\n",
    "    prev_role, temp = None, \"\"\n",
    "    for msg in example[\"messages\"]:\n",
    "        role, content = msg[\"role\"], msg[\"content\"].strip()\n",
    "        if role == prev_role:\n",
    "            temp += \" \" + content\n",
    "        else:\n",
    "            if prev_role is not None:\n",
    "                formatted[\"messages\"].append({\"role\": prev_role, \"content\": temp})\n",
    "            temp, prev_role = content, role\n",
    "    if temp:\n",
    "        formatted[\"messages\"].append({\"role\": prev_role, \"content\": temp})\n",
    "    return formatted\n",
    "\n",
    "def preprocess(example):\n",
    "    in_ids, lbls = [], []\n",
    "    msgs = example[\"messages\"]\n",
    "    for i, msg in enumerate(msgs):\n",
    "        if msg[\"role\"] != \"assistant\":\n",
    "            continue\n",
    "        # build context\n",
    "        ctx = \"\"\n",
    "        for prev in msgs[:i]:\n",
    "            tag = \"<|user|>\" if prev[\"role\"]==\"user\" else \"<|assistant|>\"\n",
    "            ctx += tag + prev[\"content\"] + tokenizer.eos_token\n",
    "        ctx += \"<|assistant|>\"\n",
    "        # response\n",
    "        resp = msg[\"content\"] + tokenizer.eos_token\n",
    "        # tokenize\n",
    "        ctx_ids  = tokenizer(ctx,  add_special_tokens=False).input_ids\n",
    "        resp_ids = tokenizer(resp, add_special_tokens=False).input_ids\n",
    "        ids      = ctx_ids + resp_ids\n",
    "        labels   = [-100]*len(ctx_ids) + resp_ids\n",
    "        # truncate\n",
    "        if len(ids) > MAX_LEN:\n",
    "            ids    = ids[-MAX_LEN:]\n",
    "            labels = labels[-MAX_LEN:]\n",
    "        in_ids.append(ids)\n",
    "        lbls.append(labels)\n",
    "    return {\"input_ids\": in_ids, \"labels\": lbls}\n",
    "\n",
    "proc = raw.map(preprocess, batched=False, remove_columns=[\"messages\"])\n",
    "all_inputs = sum(proc[\"input_ids\"], [])\n",
    "all_labels = sum(proc[\"labels\"], [])\n",
    "train_ds   = Dataset.from_dict({\"input_ids\": all_inputs, \"labels\": all_labels})\n",
    "\n",
    "# 9. collate_fn\n",
    "def collate_fn(batch):\n",
    "    input_ids = [ex[\"input_ids\"] for ex in batch]\n",
    "    labels    = [ex[\"labels\"]    for ex in batch]\n",
    "    enc = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids, \"labels\": labels},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return enc\n",
    "\n",
    "# 10. TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./solar_peft_finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=3e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 11. Trainer & 학습\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    data_collator=collate_fn\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 12. 저장\n",
    "model.save_pretrained(\"./solar_peft_finetuned\")\n",
    "tokenizer.save_pretrained(\"./solar_peft_finetuned\")\n",
    "print(\"✅ PEFT 양자화 파인튜닝 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc856ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1) Quantization 설정 (4bit)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 2) 토크나이저 로드 (파인튜닝 후 저장된 것)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./solar_peft_finetuned\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3) 베이스 모델 로드 (양자화)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"upstage/SOLAR-10.7B-v1.0\",\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 4) 임베딩 크기 재설정\n",
    "#    토크나이저의 새 vocab size (32002)에 맞춰 모델 임베딩 확장\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 5) PEFT 어댑터 로드\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./solar_peft_finetuned\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# 6) 이제 모델과 토크나이저를 써서 멀티턴 챗을 돌려 보시면 됩니다.\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"당신은 친절하고 정확한 고객상담 챗봇입니다.\"}\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in (\"exit\",\"quit\"):\n",
    "        break\n",
    "    history.append({\"role\":\"user\",\"content\":user_input})\n",
    "\n",
    "    # <|user|> / <|assistant|> 토큰 기반 프롬프트 생성\n",
    "    prompt = \"\"\n",
    "    for turn in history:\n",
    "        tag = \"<|user|>\" if turn[\"role\"]==\"user\" else \"<|assistant|>\"\n",
    "        prompt += tag + turn[\"content\"] + tokenizer.eos_token\n",
    "    prompt += \"<|assistant|>\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "    # 새로 생성된 부분만 디코딩\n",
    "    reply = tokenizer.decode(\n",
    "        outputs[0][ inputs[\"input_ids\"].shape[-1] : ],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "    print(\"Assistant:\", reply, \"\\n\")\n",
    "    history.append({\"role\":\"assistant\",\"content\":reply})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e8a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import wandb\n",
    "\n",
    "# — WANDB 설정 (한 번만 로그인)\n",
    "wandb.login()  \n",
    "\n",
    "# — 메모리 단편화 완화\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "# — 4bit 양자화 설정\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# — 기저 모델 로드 & LoRA 어댑터 장착\n",
    "model_name = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B\"\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "model.config.use_cache = False\n",
    "model.train()\n",
    "\n",
    "# — 토크나이저 세팅\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|user|>\", \"<|assistant|>\"]})\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "\n",
    "# — 데이터 로드 & 전처리\n",
    "raw = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"asia_mulit.jsonl\"},\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "def preprocess(example):\n",
    "    msgs = [{\"role\":\"system\",\"content\":\"당신은 친절하고 정확한 고객상담 챗봇입니다.\"}] + example[\"messages\"]\n",
    "    input_ids, labels = [], []\n",
    "    for i, msg in enumerate(msgs):\n",
    "        if msg[\"role\"] != \"assistant\":\n",
    "            continue\n",
    "        # 컨텍스트\n",
    "        ctx = \"\"\n",
    "        for prev in msgs[:i]:\n",
    "            tag = \"<|user|>\" if prev[\"role\"]==\"user\" else \"<|assistant|>\"\n",
    "            ctx += tag + prev[\"content\"] + tokenizer.eos_token\n",
    "        ctx += \"<|assistant|>\"\n",
    "        resp = msg[\"content\"] + tokenizer.eos_token\n",
    "\n",
    "        ctx_ids  = tokenizer(ctx,  add_special_tokens=False).input_ids\n",
    "        resp_ids = tokenizer(resp, add_special_tokens=False).input_ids\n",
    "\n",
    "        ids  = ctx_ids + resp_ids\n",
    "        lbls = [-100]*len(ctx_ids) + resp_ids\n",
    "\n",
    "        input_ids.append(ids)\n",
    "        labels.append(lbls)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "proc = raw.map(preprocess, batched=False, remove_columns=[\"messages\"])\n",
    "all_in  = sum(proc[\"input_ids\"], [])\n",
    "all_lbl = sum(proc[\"labels\"], [])\n",
    "train_ds = Dataset.from_dict({\"input_ids\": all_in, \"labels\": all_lbl})\n",
    "\n",
    "# — collate_fn\n",
    "def collate_fn(batch):\n",
    "    enc = tokenizer.pad(\n",
    "        {\"input_ids\":[b[\"input_ids\"] for b in batch],\n",
    "         \"labels\":[b[\"labels\"]     for b in batch]},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return enc\n",
    "\n",
    "# — TrainingArguments (W&B 로깅 포함)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./hyperclovax1_5B\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"hyperclovax\",\n",
    "    project=\"hyperclovax_project\",\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# — Trainer 실행\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    data_collator=collate_fn\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# — 저장\n",
    "model.save_pretrained(\"./hyperclovax1_5B\")\n",
    "tokenizer.save_pretrained(\"./hyperclovax1_5B\")\n",
    "\n",
    "print(\"✅ PEFT 양자화 파인튜닝 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d779146",
   "metadata": {},
   "source": [
    "# solar wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import wandb\n",
    "\n",
    "# — WANDB 설정 (한 번만 로그인)\n",
    "wandb.login()  \n",
    "wandb.init(\n",
    "    project=\"solar_project\",   # ← 여기에 프로젝트 이름\n",
    "    name=\"solar10_7B\"               # ← 여기에 런 이름\n",
    ")\n",
    "\n",
    "# — 메모리 단편화 완화\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "# — 4bit 양자화 설정\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# — 기저 모델 로드 & LoRA 어댑터 장착\n",
    "model_name = \"upstage/SOLAR-10.7B-Instruct-v1.0\" \n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "model.config.use_cache = False\n",
    "model.train()\n",
    "\n",
    "# — 토크나이저 세팅\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|user|>\", \"<|assistant|>\"]})\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "\n",
    "# — 데이터 로드 & 전처리\n",
    "raw = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"asia_mulit.json\"},\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "def preprocess(example):\n",
    "    msgs = [{\"role\":\"system\",\"content\":\"당신은 친절하고 정확한 고객상담 챗봇입니다.\"}] + example[\"messages\"]\n",
    "    input_ids, labels = [], []\n",
    "    for i, msg in enumerate(msgs):\n",
    "        if msg[\"role\"] != \"assistant\":\n",
    "            continue\n",
    "        # 컨텍스트\n",
    "        ctx = \"\"\n",
    "        for prev in msgs[:i]:\n",
    "            tag = \"<|user|>\" if prev[\"role\"]==\"user\" else \"<|assistant|>\"\n",
    "            ctx += tag + prev[\"content\"] + tokenizer.eos_token\n",
    "        ctx += \"<|assistant|>\"\n",
    "        resp = msg[\"content\"] + tokenizer.eos_token\n",
    "\n",
    "        ctx_ids  = tokenizer(ctx,  add_special_tokens=False).input_ids\n",
    "        resp_ids = tokenizer(resp, add_special_tokens=False).input_ids\n",
    "\n",
    "        ids  = ctx_ids + resp_ids\n",
    "        lbls = [-100]*len(ctx_ids) + resp_ids\n",
    "\n",
    "        input_ids.append(ids)\n",
    "        labels.append(lbls)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "proc = raw.map(preprocess, batched=False, remove_columns=[\"messages\"])\n",
    "all_in  = sum(proc[\"input_ids\"], [])\n",
    "all_lbl = sum(proc[\"labels\"], [])\n",
    "train_ds = Dataset.from_dict({\"input_ids\": all_in, \"labels\": all_lbl})\n",
    "\n",
    "# — collate_fn\n",
    "def collate_fn(batch):\n",
    "    enc = tokenizer.pad(\n",
    "        {\"input_ids\":[b[\"input_ids\"] for b in batch],\n",
    "         \"labels\":[b[\"labels\"]     for b in batch]},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return enc\n",
    "\n",
    "# — TrainingArguments (W&B 로깅 포함)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./solar10_7B\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=3e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"solar10_7B\",\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# — Trainer 실행\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    data_collator=collate_fn\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# — 저장\n",
    "model.save_pretrained(\"./solar10_7B\")\n",
    "tokenizer.save_pretrained(\"./solar10_7B\")\n",
    "\n",
    "print(\"✅ PEFT 양자화 파인튜닝 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a99fe",
   "metadata": {},
   "source": [
    "# wandb hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a732691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune_and_push.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import evaluate\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# ─── 0. (선택) Hugging Face 인증 ───────────────────────────────────────────────\n",
    "#  - CLI에서 한 번만: `huggingface-cli login`\n",
    "#  또는 코드로 직접 토큰 저장:\n",
    "# from huggingface_hub import HfFolder\n",
    "# HfFolder.save_token(\"hf_XXXXXXXXXXXX\")  # ← 여기에 본인의 HF token 넣기\n",
    "\n",
    "# ─── 0.1. Weights & Biases 로그인 & 초기화 ────────────\n",
    "wandb.login()  \n",
    "wandb.init(\n",
    "    project=\"solar_peft_project_asis\",      # ← 원하는 W&B 프로젝트명\n",
    "    name=\"solar_10.asia\"    # ← 이 실험의 런 이름\n",
    ")\n",
    "\n",
    "# ─── 1. 메모리 단편화 완화 ────────────────────────────────────────────────────\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "# ─── 2. 4bit 양자화 설정 ────────────────────────────────────────────────────\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# ─── 3. 기본 모델 로드 및 LoRA 어댑터 부착 ────────────────────────────────────\n",
    "model_name = \"upstage/SOLAR-10.7B-v1.0\"\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "# 캐시 끄기 + gradient checkpointing (메모리 여유시 켜기)\n",
    "model.config.use_cache = False\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "model.train()\n",
    "\n",
    "# 멀티 GPU 설정\n",
    "#if torch.cuda.device_count() > 1:\n",
    "#   model = torch.nn.DataParallel(model)\n",
    "    \n",
    "# ─── 4. 학습 가능 파라미터 확인 (디버그용) ───────────────────────────────────\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"▶︎ Trainable params: {trainable_params:,} / {total_params:,}\")\n",
    "\n",
    "# ─── 5. 토크나이저 세팅 ───────────────────────────────────────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|user|>\", \"<|assistant|>\"]})\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "\n",
    "# ─── 6. 데이터 로드 & 전처리 ─────────────────────────────────────────────────\n",
    "raw = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"asia_mulit.json\"},\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "#MAX_LEN = 256\n",
    "\n",
    "def preprocess(example):\n",
    "    # system 프롬프트 + 연속 같은 role 메시지 병합\n",
    "    msgs = [{\"role\":\"system\",\"content\":\"\"\"\n",
    "    당신은 “국립아시아문화전당 공식 고객상담 챗봇”입니다.\n",
    "아래의 지침을 따라, 멀티턴 대화에서 사용자의 문의에 친절하고 정확하게 답변하세요.\n",
    "\n",
    "1. **페르소나 & 톤**\n",
    "   - 정중하면서도 따뜻한 말투를 사용합니다.\n",
    "   - 고객의 상황에 공감하고, 안심시킬 수 있는 표현을 우선시합니다.\n",
    "   - 절대로 기계적·딱딱한 어투를 쓰지 않습니다.\n",
    "\n",
    "2. **도메인 지식 & 제약조건**\n",
    "   - 회원/비회원 예매 조회, 단체 예약, 취소·환불 규정, 수수료 정책 등\n",
    "   - 예매 방법(홈페이지·콜센터·현장 매표소)별 차이와 제한 사항\n",
    "   - 공연명, 공연 일시, 인원, 연락처 등 고객이 제공한 정보를 반드시 반영\n",
    "   - 최신 운영 시간·정책(예: 티켓 오픈 시간, 취소 수수료 발생 시점)을 정확히 안내\n",
    "\n",
    "3. **응답 구조 (멀티턴 Q&A)**\n",
    "   매 턴마다 아래 순서대로 답변을 구성합니다.\n",
    "   1) **핵심 정보**: 사용자의 직접 질문에 대한 간결한 답변  \n",
    "   2) **추가 안내**: 필요한 세부 설명이나 주의사항  \n",
    "   3) **추천 행동**: 다음에 고객이 취해야 할 구체적인 단계를 번호 매겨 제시  \n",
    "\n",
    "4. **멀티턴 흐름 유지**\n",
    "   - 고객의 이전 질문을 항상 기억하고 문맥을 반영하세요.\n",
    "   - 사용자가 추가 정보를 제공하면, 반드시 그 정보(예: “7월 3일 오셀로”)를 답변에 포함합니다.\n",
    "   - 대화가 길어져도 “핵심→추가 안내→추천 행동” 구조를 매번 유지합니다.\n",
    "    \"\"\"}]\n",
    "    prev_role, buf = None, \"\"\n",
    "    for m in example[\"messages\"]:\n",
    "        role, text = m[\"role\"], m[\"content\"].strip()\n",
    "        if role == prev_role:\n",
    "            buf += \" \" + text\n",
    "        else:\n",
    "            if prev_role:\n",
    "                msgs.append({\"role\":prev_role, \"content\":buf})\n",
    "            buf, prev_role = text, role\n",
    "    if buf:\n",
    "        msgs.append({\"role\":prev_role, \"content\":buf})\n",
    "    # assistant 응답마다 context+label 생성\n",
    "    input_ids, labels = [], []\n",
    "    for i, m in enumerate(msgs):\n",
    "        if m[\"role\"] != \"assistant\": continue\n",
    "        # context\n",
    "        ctx = \"\"\n",
    "        for prev in msgs[:i]:\n",
    "            tag = \"<|user|>\" if prev[\"role\"]==\"user\" else \"<|assistant|>\"\n",
    "            ctx += tag + prev[\"content\"] + tokenizer.eos_token\n",
    "        ctx += \"<|assistant|>\"\n",
    "        # resp\n",
    "        resp = msgs[i][\"content\"] + tokenizer.eos_token\n",
    "        ctx_ids = tokenizer(ctx, add_special_tokens=False).input_ids\n",
    "        rsp_ids = tokenizer(resp, add_special_tokens=False).input_ids\n",
    "        ids = ctx_ids + rsp_ids\n",
    "        lbl = [-100]*len(ctx_ids) + rsp_ids\n",
    "        # truncate\n",
    "        # if len(ids) > MAX_LEN:\n",
    "        #     ids = ids[-MAX_LEN:]\n",
    "        #     lbl = lbl[-MAX_LEN:]\n",
    "        input_ids.append(ids)\n",
    "        labels.append(lbl)\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "proc = raw.map(preprocess, batched=False, remove_columns=[\"messages\"])\n",
    "all_in = sum(proc[\"input_ids\"], [])\n",
    "all_lb = sum(proc[\"labels\"], [])\n",
    "train_ds = Dataset.from_dict({\"input_ids\": all_in, \"labels\": all_lb})\n",
    "\n",
    "accuracy_metric  = evaluate.load(\"accuracy\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # eval_pred: (logits, labels)\n",
    "    logits, labels = eval_pred\n",
    "    # ——— 토큰 단위 Accuracy ———\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    mask  = labels != -100\n",
    "    acc = accuracy_metric.compute(\n",
    "        predictions=preds[mask].flatten().tolist(),\n",
    "        references=labels[mask].flatten().tolist()\n",
    "    )[\"accuracy\"]\n",
    "\n",
    "    # ——— 시퀀스 디코딩 후 BERTScore(유사도) ———\n",
    "    # (skip_special_tokens=True 로 본문만)\n",
    "    pred_strs  = tokenizer.batch_decode(preds,  skip_special_tokens=True)\n",
    "    label_strs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    bert = bertscore_metric.compute(\n",
    "        predictions=pred_strs,\n",
    "        references=label_strs,\n",
    "        lang=\"ko\"\n",
    "    )\n",
    "    # f1 점수 평균\n",
    "    bert_f1 = sum(bert[\"f1\"]) / len(bert[\"f1\"])\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"bertscore_f1\": bert_f1\n",
    "    }\n",
    "\n",
    "\n",
    "# ─── 7. collate_fn 정의 ─────────────────────────────────────────────────────\n",
    "def collate_fn(batch):\n",
    "    in_ids = [ex[\"input_ids\"] for ex in batch]\n",
    "    lbs   = [ex[\"labels\"]    for ex in batch]\n",
    "    return tokenizer.pad(\n",
    "        {\"input_ids\": in_ids, \"labels\": lbs},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# ─── 8. TrainingArguments & Trainer 초기화 ──────────────────────────────────\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./solar_peft_finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=3e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    report_to = \"wandb\"\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ─── 9. 학습 시작 ────────────────────────────────────────────────────────────\n",
    "trainer.train()\n",
    "\n",
    "# ─── 10. 로컬 저장 ──────────────────────────────────────────────────────────\n",
    "model.save_pretrained(\"./solar_peft_finetuned\")\n",
    "tokenizer.save_pretrained(\"./solar_peft_finetuned\")\n",
    "print(\"✅ 로컬에 모델·토크나이저 저장 완료\")\n",
    "\n",
    "# ─── 11. Hugging Face Hub에 업로드 ───────────────────────────────────────────\n",
    "from huggingface_hub import HfApi, Repository, HfFolder\n",
    "\n",
    "# (이미 로그인했다면 아래 줄은 생략 가능)\n",
    "# HfFolder.save_token(\"hf_XXXXXXXXXXXX\")  # ← HF 토큰을 직접 삽입해도 됩니다\n",
    "\n",
    "api = HfApi()\n",
    "repo_id = \"seoungji/solar-peft-finetuned\"  # ← \"hf_계정명/리포명\" 으로 수정\n",
    "# 한 번만 리포지터리 생성 (private=True로 설정 시 비공개)\n",
    "api.create_repo(name=repo_id.split(\"/\")[-1],\n",
    "                token=HfFolder.get_token(),\n",
    "                private=False)\n",
    "\n",
    "# 로컬 폴더와 원격 리포 연결\n",
    "repo = Repository(\n",
    "    local_dir=\"./solar_peft_finetuned\",\n",
    "    clone_from=repo_id,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "# 커밋 & 푸시\n",
    "repo.push_to_hub(commit_message=\"Add fine-tuned 4bit LoRA model\")\n",
    "print(f\"✅ 모델 업로드 완료: https://huggingface.co/{repo_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
