# rag_api.py

from typing import TypedDict, List, Annotated
from langchain_core.documents import Document
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langgraph.graph import StateGraph
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings  # âœ… ë³€ê²½
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
from fastapi import FastAPI
from pydantic import BaseModel
import torch
import os

# â”€â”€â”€ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

# â”€â”€â”€ FAISS ë²¡í„°DB ë° ìž„ë² ë”© ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large-instruct")
vectorstore = FAISS.load_local("card", embedding_model, allow_dangerous_deserialization=True)

# â”€â”€â”€ Midm ëª¨ë¸ ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
midm_model_name = "K-intelligence/Midm-2.0-Base-Instruct"
midm_tokenizer = AutoTokenizer.from_pretrained(midm_model_name)
midm_model = AutoModelForCausalLM.from_pretrained(
    midm_model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)
midm_generation_config = GenerationConfig.from_pretrained(midm_model_name)

# â”€â”€â”€ Midm ì‘ë‹µ ìƒì„± í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_answer(decoded: str) -> str:
    """
    Midm ì¶œë ¥ì—ì„œ ì‹¤ì œ assistant ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    """
    if "assistant\n" in decoded:
        return decoded.split("assistant\n")[-1].strip()
    elif "assistant:" in decoded:
        return decoded.split("assistant:")[-1].strip()
    elif "ë‹µë³€:" in decoded:
        return decoded.split("ë‹µë³€:")[-1].strip()
    else:
        return decoded.strip().splitlines()[-1]


def generate_with_midm(question: str, context: str) -> str:
    system_prompt = f"""
ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ì‚¬ìš©ìž ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”:

    ë„ˆëŠ” ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.
     ë‹µë³€ì—ëŠ” 'assistant:'ë‚˜ 'user:' ê°™ì€ ë‹¨ì–´ë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆ.
     ë¬¸ë‹µ í˜•ì‹ ì—†ì´ ìžì—°ì–´ë¡œë§Œ ë‹µë³€í•´.
     ì˜¤ì§ ì§ˆë¬¸ì— ëŒ€í•œ í•µì‹¬ ì •ë³´ë§Œ ì „ë‹¬í•´.
     ë§ºëŠ”ë§ì´ë‚˜ ì¸ì‚¿ë§ ì—†ì´, ì •ì¤‘í•œ ì¡´ëŒ“ë§ë¡œ ë‹µí•´.
    """

    # user_prompt = f"ì§ˆë¬¸: {question}\nìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜."

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"ë¬¸ì„œ: {context}\nì§ˆë¬¸: {question}\nìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ë‹µë³€í•´ì¤˜."}
    ]

    prompt_text = midm_tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    input_ids = midm_tokenizer(prompt_text, return_tensors="pt").input_ids.to("cuda")
    attention_mask = input_ids.ne(midm_tokenizer.pad_token_id)

    output = midm_model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_new_tokens=1024,
        do_sample=True,
        temperature=0.8,
        top_p=0.75,
        top_k=20,
        pad_token_id=midm_tokenizer.pad_token_id,
        eos_token_id=midm_tokenizer.eos_token_id
    )

    decoded = midm_tokenizer.decode(output[0], skip_special_tokens=True)
    final_answer = decoded

    print("âœ… ìµœì¢… ë‹µë³€:", final_answer)
    return final_answer





    # âœ… attention_maskì™€ pad_token_id ì„¤ì • (ê²½ê³  ë°©ì§€ ë° ì•ˆì •ì  ì¶œë ¥)
    attention_mask = input_ids.ne(midm_tokenizer.pad_token_id)

    output = midm_model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,                    # ðŸ”§ ì¶”ê°€
        max_new_tokens=1024,
        do_sample=True,                                   # ðŸ”§ ìƒ˜í”Œë§ ì¼œê¸°
        temperature=0.8,
        top_p=0.75,
        top_k=20,
        pad_token_id=midm_tokenizer.pad_token_id,         # ðŸ”§ ì¶”ê°€
        eos_token_id=midm_tokenizer.eos_token_id
    )

    decoded = midm_tokenizer.decode(output[0], skip_special_tokens=True)

    # âœ… ë””ë²„ê¹…ìš© ì¶œë ¥
    print("ðŸ” Decoded output:", decoded)

    if not decoded.strip():
        print("âŒ ëª¨ë¸ì´ ë¹ˆ ì‘ë‹µì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤. Prompt í™•ì¸ í•„ìš”")

    return extract_answer(decoded)


# â”€â”€â”€ LangGraph State ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class GraphState(TypedDict):
    question: Annotated[str, "ì§ˆë¬¸"]
    answer: Annotated[str, "ë‹µë³€"]
    score: Annotated[float, "ìœ ì‚¬ë„ ì ìˆ˜"]
    retriever_docs: Annotated[List[Document], "ìœ ì‚¬ë„ ìƒìœ„ë¬¸ì„œ"]

# â”€â”€â”€ LangGraph ë…¸ë“œ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def retriever_node(state: GraphState) -> GraphState:
#     docs = vectorstore.similarity_search_with_score(state["question"], k=3)
#     retrieved_docs = [doc for doc, _ in docs]
#     score = docs[0][1]
#     print("[question]:", state["question"])
#     print("[retriever_node] ìœ ì‚¬ë„:", score)
#     return {
#         "question": state["question"],
#         "retriever_docs": retrieved_docs,
#         "score": score,
#         "answer": ""
#     }

# 2. ë…¸ë“œ ì •ì˜
def retriever_node(state: GraphState) -> GraphState:
    docs = vectorstore.similarity_search_with_score(state["question"], k=3)
    retrieved_docs = [doc for doc, _ in docs]
    score = docs[0][1]
    # print("\n[retriever_node] ë¬¸ì„œ:", [doc.page_content for doc in retrieved_docs])
    print("[queston]:", state["question"])
    print("[retriever_node] ìƒìœ„ ë¬¸ì„œì˜ ì œëª©:\n",docs)
    print("[retriever_node] ìœ ì‚¬ë„ ì ìˆ˜:", score)
    # print('-'*100)
    # print(docs)
    # print('-'*100)
    # print(retrieved_docs[0])
    return GraphState(score=score, retriever_docs=retrieved_docs)



def grade_documents_node(state: GraphState) -> GraphState:
    return GraphState()

def llm_answer_node(state: GraphState) -> GraphState:
    docs_content = "\n---\n".join([doc.page_content for doc in state["retriever_docs"]])
    answer = generate_with_midm(state["question"], docs_content)
    
    # âœ… í•œ êµ°ë°ì—ì„œë§Œ ì¶œë ¥
    print("ðŸ¤– ìµœì¢… LLM ì‘ë‹µ:", answer)

    return {
        "question": state["question"],
        "retriever_docs": state["retriever_docs"],
        "score": state["score"],
        "answer": answer
    }



def query_rewrite_node(state: GraphState) -> GraphState:
    prompt = f"ì§ˆë¬¸: {state['question']}\nìœ„ ì§ˆë¬¸ì˜ í•µì‹¬ì€ ìœ ì§€í•˜ë©´ì„œ, ìœ ì‚¬ ë¬¸ì„œë¥¼ ë” ìž˜ ì°¾ì„ ìˆ˜ ìžˆë„ë¡ ì§ˆë¬¸ì„ ë‹¤ì‹œ ì¨ì¤˜."
    messages = [
        # {"role": "system", "content": "ë‹¹ì‹ ì€ ì§ˆë¬¸ì„ ë¦¬ë¼ì´íŒ…í•´ì£¼ëŠ” ë„ìš°ë¯¸ìž…ë‹ˆë‹¤."},
        {"role": "user", "content": prompt}
    ]

    input_ids = midm_tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to("cuda")

    output = midm_model.generate(
        input_ids,
        generation_config=midm_generation_config,
        max_new_tokens=512,
        top_p=0.75,
        do_sample=False,
        eos_token_id=midm_tokenizer.eos_token_id
    )

    new_question = midm_tokenizer.decode(output[0], skip_special_tokens=True)
    print("[query_rewrite_node] ë¦¬ë¼ì´íŒ…:", new_question)
    return {
        "question": new_question,
        "retriever_docs": [],
        "score": 0.0,
        "answer": ""
    }

def decide_to_generate(state: GraphState) -> str:
    return "llm_answer" if state["score"] <= 0.5 else "query_rewrite"

# â”€â”€â”€ LangGraph êµ¬ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
workflow = StateGraph(GraphState)
workflow.add_node("retriever", retriever_node)
workflow.add_node("llm_answer", llm_answer_node)
workflow.add_node("grade_documents", grade_documents_node)
workflow.add_node("query_rewrite", query_rewrite_node)
workflow.set_entry_point("retriever")
workflow.add_conditional_edges("retriever", decide_to_generate, {
    "llm_answer": "llm_answer",
    "query_rewrite": "query_rewrite"
})
workflow.add_edge("query_rewrite", "retriever")
app_graph = workflow.compile()

# â”€â”€â”€ ì§€ì†ì ì¸ ë©€í‹°í„´ ëŒ€í™” ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ðŸ§  sLLM RAG ì±—ë´‡ì— ì˜¤ì‹  ê±¸ í™˜ì˜í•©ë‹ˆë‹¤! 'exit'ì„ ìž…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.\n")

conversation_history = []

while True:
    user_input = input("ðŸ™‹ ì‚¬ìš©ìž ì§ˆë¬¸: ").strip()
    if user_input.lower() in {"exit", "quit", "ì¢…ë£Œ"}:
        print("ðŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    # LangGraph íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    response = app_graph.invoke({
        "question": user_input,
        "answer": "",
        "score": 0.0,
        "retriever_docs": []
    })

    assistant_answer = response.get("answer", "")
    print(f"ðŸ¤– sLLM ì‘ë‹µ: {assistant_answer}\n")

    # í•„ìš”í•˜ë‹¤ë©´ history ì €ìž¥
    conversation_history.append({
        "user": user_input,
        "assistant": assistant_answer
    })

