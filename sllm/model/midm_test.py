import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# â”€â”€â”€ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model_name = "K-intelligence/Midm-2.0-Base-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)
generation_config = GenerationConfig.from_pretrained(model_name)

# â”€â”€â”€ Vector DB ë¡œë“œ (FAISS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large-instruct")
vectorstore = FAISS.load_local("./vectordb", embedding_model, allow_dangerous_deserialization=True)

# â”€â”€â”€ ëŒ€í™” ë£¨í”„ ì‹œì‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ’¬ VectorDB ê¸°ë°˜ Midm ì±—ë´‡ì…ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit' ì…ë ¥.\n")

chat_history = []

while True:
    user_input = input("ğŸ™‹ ì‚¬ìš©ì: ")

    if user_input.lower() in ["exit", "quit"]:
        print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    # ë¬¸ìì—´ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜ + ê³µë°± ì œê±°
    query = str(user_input).strip()

    # ë¹ˆ ì…ë ¥ ë°©ì§€
    if not query:
        print("â— ìœ íš¨í•œ ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.\n")
        continue

    # ğŸ” ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ì˜ˆì™¸ ë°©ì§€)
    try:
        results = vectorstore.similarity_search(query, k=3)
        context = "\n".join([doc.page_content for doc in results]) if results else "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        continue

    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì— context ì‚½ì…
    context_message = {
        "role": "system",
        "content": f"""
1. **ì¹œì ˆí•œ ì¸ì‚¬**  
   - ì‚¬ìš©ìê°€ ì²˜ìŒ ì§ˆë¬¸í•  ë•ŒëŠ” â€œì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?â€ ë“±ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.

2. **ëª…í™•í•œ ì´í•´ í™•ì¸**  
   - ì§ˆë¬¸ì˜ ì˜ë„ê°€ ëª¨í˜¸í•˜ë©´, â€œì£„ì†¡í•˜ì§€ë§Œ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì •ë³´ë¥¼ ì°¾ìœ¼ì‹œëŠ”ì§€ ì•Œë ¤ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?â€ë¼ê³  ì—¬ì­¤ë³´ì„¸ìš”.

3. **í•œê¸€Â·ì •ì¤‘ì²´ ìœ ì§€**  
   - ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³  í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì¤˜.

4. **ì¶”ê°€ ì•ˆë‚´**  
   - ë‹µë³€ í›„ â€œë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.â€ì™€ ê°™ì´ ëŒ€í™”ë¥¼ ì´ì–´ê°ˆ ì—¬ì§€ë¥¼ ë‚¨ê¸°ì„¸ìš”.

5. **ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ**  
   - ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

[ì°¸ê³  ì •ë³´]
{context}
"""
    }

    # ë©”ì‹œì§€ êµ¬ì„±
    current_messages = [context_message, {"role": "user", "content": query}]

    input_ids = tokenizer.apply_chat_template(
        current_messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to("cuda")

    # ëª¨ë¸ ì‘ë‹µ ìƒì„±
    output = model.generate(
        input_ids,
        generation_config=generation_config,
        eos_token_id=tokenizer.eos_token_id,
        max_new_tokens=256,
        do_sample=False,
    )

    # ì‘ë‹µ í›„ì²˜ë¦¬
    response_text = tokenizer.decode(output[0], skip_special_tokens=True)
    assistant_reply = response_text.split("<|assistant|>\n")[-1].strip()

    print(f"ğŸ¤– Mi:dm: {assistant_reply}\n")

    # ëŒ€í™” ê¸°ë¡ ì €ì¥
    chat_history.append({"role": "user", "content": query})
    chat_history.append({"role": "assistant", "content": assistant_reply})
