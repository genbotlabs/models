import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from evaluate import load
from tqdm import tqdm
from torch.utils.data import DataLoader
from transformers import AutoModelForSequenceClassification, AutoTokenizer as DRTTokenizer

# â”€â”€â”€ 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model_path = "./google_gemma-3-1B"
model = AutoModelForCausalLM.from_pretrained(model_path).to("cuda")
tokenizer = AutoTokenizer.from_pretrained(model_path)
model.eval()

# â”€â”€â”€ 2. í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
eval_data = load_dataset("json", data_files="VS_random_child.jsonl", split="train")

# â”€â”€â”€ 3. ì „ì²˜ë¦¬ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def preprocess(example):
    msgs = [{"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì •í™•í•œ ê³ ê°ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤."}]
    msgs += example["messages"]
    input_ids, labels = [], []

    for i, m in enumerate(msgs):
        if m["role"] != "assistant":
            continue
        context = "".join(
            ("<|user|>" if prev["role"] == "user" else "<|assistant|>")
            + prev["content"] + tokenizer.eos_token
            for prev in msgs[:i]
        )
        context += "<|assistant|>"
        response = m["content"] + tokenizer.eos_token
        ctx_ids = tokenizer(context, add_special_tokens=False).input_ids
        rsp_ids = tokenizer(response, add_special_tokens=False).input_ids
        ids = ctx_ids + rsp_ids
        lbl = [-100] * len(ctx_ids) + rsp_ids
        input_ids.append(ids)
        labels.append(lbl)

    return {"input_ids": input_ids, "labels": labels}

# â”€â”€â”€ 4. ì „ì²˜ë¦¬ ë° ë°ì´í„° ì¤€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
proc = eval_data.map(preprocess, batched=False, remove_columns=["messages"])
all_inputs = sum(proc["input_ids"], [])
all_labels = sum(proc["labels"], [])

# â”€â”€â”€ 5. ë°°ì¹˜ í‰ê°€ìš© DataLoader êµ¬ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BATCH_SIZE = 1
tokenized_inputs = tokenizer.pad({"input_ids": all_inputs}, padding=True, return_tensors="pt")
tokenized_labels = tokenizer.pad({"input_ids": all_labels}, padding=True, return_tensors="pt")

dataset = torch.utils.data.TensorDataset(tokenized_inputs["input_ids"], tokenized_labels["input_ids"])
loader = DataLoader(dataset, batch_size=BATCH_SIZE)

preds_all, labels_all = [], []
with torch.no_grad():
    for input_ids, label_ids in tqdm(loader, desc="Running batches"):
        input_ids = input_ids.to("cuda")
        outputs = model(input_ids=input_ids)
        logits = outputs.logits
        preds = logits.argmax(dim=-1).cpu()
        preds_all.extend(preds)
        labels_all.extend(label_ids)

# â”€â”€â”€ 6. í‰ê°€ ì§€í‘œ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â¬› Accuracy: í† í° ì •í™•ë„
accuracy_metric = load("accuracy")
# â¬› BERTScore: ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ë„
bertscore_metric = load("bertscore")
# â¬› BLEU: n-gram ê¸°ê³„ë²ˆì—­ ê¸°ë°˜ ì •ëŸ‰ ì ìˆ˜
bleu_metric = load("bleu")
# â¬› ROUGE: ìš”ì•½ í‰ê°€ìš© longest overlap
rouge_metric = load("rouge")

preds_all = torch.stack(preds_all).tolist()
labels_all = torch.stack(labels_all).tolist()

# â”€â”€ ë§ˆìŠ¤í‚¹ ì œê±°
flat_preds, flat_labels = [], []
for p, l in zip(preds_all, labels_all):
    for pi, li in zip(p, l):
        if li != -100:
            flat_preds.append(pi)
            flat_labels.append(li)

acc = accuracy_metric.compute(predictions=flat_preds, references=flat_labels)["accuracy"]

# â”€â”€ í…ìŠ¤íŠ¸ ë””ì½”ë”©
pred_strs = tokenizer.batch_decode(preds_all, skip_special_tokens=True)
label_strs = tokenizer.batch_decode(labels_all, skip_special_tokens=True)

bert = bertscore_metric.compute(predictions=pred_strs, references=label_strs, lang="ko")
bleu = bleu_metric.compute(
    predictions=[p.split() for p in pred_strs],
    references=[[l.split()] for l in label_strs]
)
rouge = rouge_metric.compute(predictions=pred_strs, references=label_strs)

# â”€â”€â”€ 7. DialogRPT í‰ê°€ (ì‚¬ì „í•™ìŠµëœ ëŒ€í™” ì ì ˆì„± í‰ê°€ëª¨ë¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
dialogrpt_tokenizer = DRTTokenizer.from_pretrained("microsoft/DialogRPT-updown")
dialogrpt_model = AutoModelForSequenceClassification.from_pretrained("microsoft/DialogRPT-updown").to("cuda")
dialogrpt_model.eval()

scores = []
for ctx, rsp in tqdm(zip(label_strs, pred_strs), total=len(pred_strs), desc="DialogRPT"):
    text = ctx + " " + rsp
    enc = dialogrpt_tokenizer(text, return_tensors="pt", truncation=True, padding=True).to("cuda")
    with torch.no_grad():
        logit = dialogrpt_model(**enc).logits.squeeze().item()
    scores.append(logit)

dialogrpt_avg = sum(scores) / len(scores)

# â”€â”€â”€ 8. ê²°ê³¼ ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ“Š Evaluation Results")
print(f"DialogRPT (avg):        {dialogrpt_avg:.4f}   # ì‚¬ì „í•™ìŠµ ê¸°ë°˜ ì‘ë‹µ ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€")
print(f"Accuracy:               {acc:.4f}             # í† í° ë‹¨ìœ„ ì •í™•ë„")
print(f"BERTScore F1:           {sum(bert['f1']) / len(bert['f1']):.4f}   # ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ë„")
print(f"BLEU:                   {bleu['bleu']:.4f}     # n-gram ì •ëŸ‰ ë¹„êµ")
print(f"ROUGE-L:                {rouge['rougeL']:.4f}  # ìš”ì•½ í‰ê°€ ê¸°ë°˜")
